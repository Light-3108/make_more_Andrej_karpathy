{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0dc71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "******* Exercise - 4**************\n",
    "\n",
    "# what i have to do\n",
    "# 1. we did f.one_hot which just makes input of 1 * (27*27) i.e 0 0 0 0 .... 1..0000 .\n",
    "# 2. seems time_consuming to do this for every training example as we can just select the corresponding row in W.\n",
    "\n",
    "# Ok let's do this. \n",
    "# it's simple just do logits = W[X_train].  (simple matrix multiplication)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3cdc04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# W = torch.rand(10,3)\n",
    "# X_train = torch.tensor([2,5,8,1])\n",
    "# xenc = F.one_hot(X_train, num_classes = 10).float()\n",
    "# print(W[X_train])\n",
    "# print(xenc@W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c42aa213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yes they are same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9cd78b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "words = open('names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c0b330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c11162b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "655d494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  196113\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append((ix1*27)+ix2)\n",
    "    ys.append(ix3)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "55a8687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_temp,y_train,y_temp = train_test_split(xs,ys,test_size = 0.2, random_state=42)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp,y_temp, test_size=0.5, random_state = 42)\n",
    "\n",
    "# X_train, y_train ma 80% data xa ani\n",
    "#X_dev ra X_test ma 10 10% data xa ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "888a99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.size())\n",
    "# print(X_dev.size())\n",
    "# print(X_test.size())\n",
    "train_size = len(X_train)\n",
    "dev_size = len(X_dev)\n",
    "test_size = len(y_test)\n",
    "# X_train = X_train.float()\n",
    "# y_train = y_train.float()\n",
    "# X_dev = X_dev.float()\n",
    "# y_dev = y_dev.float()\n",
    "# y_train = y_train.float()\n",
    "# y_test = y_test.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1af89585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW i have to create a variable to store the dev_loss every time. if at some point dev_loss(new) > dev_loss(past) \n",
    "#stop the training\n",
    "\n",
    "#Also let's find after how many iteration we did early stopping\n",
    "#Also let's see if doing early stopping make our results better\n",
    "\n",
    "dev_loss = 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bc4948b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ffa9f938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_loss: 2.3962433338165283\n",
      "loss: 2.4097914695739746\n",
      "dev_loss: 2.393950939178467\n",
      "loss: 2.4074549674987793\n",
      "dev_loss: 2.3916923999786377\n",
      "loss: 2.4051523208618164\n",
      "dev_loss: 2.389467239379883\n",
      "loss: 2.402883291244507\n",
      "dev_loss: 2.3872740268707275\n",
      "loss: 2.400646924972534\n",
      "dev_loss: 2.385112762451172\n",
      "loss: 2.398442268371582\n",
      "dev_loss: 2.3829824924468994\n",
      "loss: 2.3962695598602295\n",
      "dev_loss: 2.380882501602173\n",
      "loss: 2.394127130508423\n",
      "dev_loss: 2.378812074661255\n",
      "loss: 2.392014503479004\n",
      "dev_loss: 2.3767707347869873\n",
      "loss: 2.3899309635162354\n",
      "dev_loss: 2.374757766723633\n",
      "loss: 2.387876510620117\n",
      "dev_loss: 2.372772455215454\n",
      "loss: 2.385850667953491\n",
      "dev_loss: 2.370814800262451\n",
      "loss: 2.3838515281677246\n",
      "dev_loss: 2.3688836097717285\n",
      "loss: 2.381880044937134\n",
      "dev_loss: 2.366978645324707\n",
      "loss: 2.379934310913086\n",
      "dev_loss: 2.3650989532470703\n",
      "loss: 2.3780152797698975\n",
      "dev_loss: 2.3632445335388184\n",
      "loss: 2.3761210441589355\n",
      "dev_loss: 2.361414909362793\n",
      "loss: 2.3742518424987793\n",
      "dev_loss: 2.3596091270446777\n",
      "loss: 2.3724076747894287\n",
      "dev_loss: 2.3578271865844727\n",
      "loss: 2.370586633682251\n",
      "dev_loss: 2.3560686111450195\n",
      "loss: 2.3687894344329834\n",
      "dev_loss: 2.354332447052002\n",
      "loss: 2.3670151233673096\n",
      "dev_loss: 2.35261869430542\n",
      "loss: 2.3652637004852295\n",
      "dev_loss: 2.350926399230957\n",
      "loss: 2.3635342121124268\n",
      "dev_loss: 2.3492558002471924\n",
      "loss: 2.3618266582489014\n",
      "dev_loss: 2.3476061820983887\n",
      "loss: 2.360140085220337\n",
      "dev_loss: 2.345977783203125\n",
      "loss: 2.3584744930267334\n",
      "dev_loss: 2.3443691730499268\n",
      "loss: 2.3568294048309326\n",
      "dev_loss: 2.342780351638794\n",
      "loss: 2.3552043437957764\n",
      "dev_loss: 2.3412113189697266\n",
      "loss: 2.3535993099212646\n",
      "dev_loss: 2.3396615982055664\n",
      "loss: 2.3520140647888184\n",
      "dev_loss: 2.338130235671997\n",
      "loss: 2.350447654724121\n",
      "dev_loss: 2.3366177082061768\n",
      "loss: 2.3488993644714355\n",
      "dev_loss: 2.3351235389709473\n",
      "loss: 2.347369909286499\n",
      "dev_loss: 2.333646535873413\n",
      "loss: 2.345858573913574\n",
      "dev_loss: 2.3321871757507324\n",
      "loss: 2.344364881515503\n",
      "dev_loss: 2.3307456970214844\n",
      "loss: 2.342888355255127\n",
      "dev_loss: 2.3293204307556152\n",
      "loss: 2.3414292335510254\n",
      "dev_loss: 2.327911853790283\n",
      "loss: 2.339986801147461\n",
      "dev_loss: 2.32651948928833\n",
      "loss: 2.3385605812072754\n",
      "dev_loss: 2.325143814086914\n",
      "loss: 2.337151050567627\n",
      "dev_loss: 2.3237831592559814\n",
      "loss: 2.3357574939727783\n",
      "dev_loss: 2.3224380016326904\n",
      "loss: 2.334379196166992\n",
      "dev_loss: 2.321108102798462\n",
      "loss: 2.3330161571502686\n",
      "dev_loss: 2.319793701171875\n",
      "loss: 2.3316686153411865\n",
      "dev_loss: 2.318493127822876\n",
      "loss: 2.330335855484009\n",
      "dev_loss: 2.3172075748443604\n",
      "loss: 2.329017400741577\n",
      "dev_loss: 2.3159360885620117\n",
      "loss: 2.3277134895324707\n",
      "dev_loss: 2.314678430557251\n",
      "loss: 2.3264236450195312\n",
      "dev_loss: 2.313434362411499\n",
      "loss: 2.3251473903656006\n"
     ]
    }
   ],
   "source": [
    "for k in range(50):\n",
    "    \n",
    "    \n",
    "    #forward pass for train-set\n",
    "    \n",
    "#     xenc = F.one_hot(X_train, num_classes = (27*27)).float() #input to the network: one-hot encoding\n",
    "#     logits = no_of_examples * 27\n",
    "    \n",
    "    logits = W[X_train]  #log-counts predict gareko hai\n",
    "    counts = logits.exp()  #coutns, equivalent to N\n",
    "    probs = counts/counts.sum(1,keepdim = True) #probabilities for next character\n",
    "    \n",
    "    loss = -probs[torch.arange(train_size),y_train].log().mean() + 0.01 * (W**2).mean()  # i haven't understood this second part\n",
    "    \n",
    "    \n",
    "    #forward pass for dev-set\n",
    "    logits_dev = W[X_dev]\n",
    "    counts_dev = logits_dev.exp()\n",
    "    probs_dev = counts_dev/ counts_dev.sum(1, keepdim = True)\n",
    "    loss_2 = -probs_dev[torch.arange(dev_size),y_dev].log().mean()\n",
    "    \n",
    "    print(f\"dev_loss: {loss_2.item()}\")\n",
    "    \n",
    "    if(dev_loss < loss_2):\n",
    "        print(f\"early stopping done, stoped at: {k}\")\n",
    "        break\n",
    "    else:\n",
    "        dev_loss = loss_2\n",
    "    #maile bujeko yo loss chai,\n",
    "#     probs[torch.arrange(num),ys] this gives probability predcited by our model for ground truth. if it is less than \n",
    "#     100% model we give it a loss i.e -loglikelihood now model updates it's parameters.\n",
    "    \n",
    "#     Q. I am super-amazed how this loss worked? \n",
    "#     --> because we are not always sure what letters comes after pairs in real names. any letter can come. \n",
    "#         but some are more likely.\n",
    "#     --> i think our model will twiks our W such that it gives more probabilty for more likely words based on training set\n",
    "    \n",
    "#     Q. Still confused how loss will converged? I suspect it will not. (noob me, think later)\n",
    "#     Q. \n",
    "\n",
    "     \n",
    "    print(f\"loss: {loss.item()}\")\n",
    "    #backward pass:\n",
    "    \n",
    "    W.grad = None # paile sabaile gradient lai zero gareko\n",
    "    loss.backward()  #magic hehe\n",
    "    \n",
    "    #update\n",
    "    \n",
    "    W.data += -50*W.grad   #remember to increment opp to direction of gradient, otherwise face the consequences)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "daba6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i noticed if i change 0.01 to 0.0001 network learns slower\n",
    "# ok this is not the case i think, does this have any advantages?\n",
    "\n",
    "# if multiplicating factor(K)is too big, faster overfitting which helps to generalize the data?\n",
    "#  if we decrease K, penalty term becomes less significant. so model may overfitt. (meaning good on train, bad on test)\n",
    "#  what about small k with early stopping?\n",
    "\n",
    "# ---> because we are using early stopping to prevent model from overfitting and using small k to get high accuracy?\n",
    "\n",
    "# is this right?\n",
    "\n",
    "# what i don't know\n",
    "# Q.   What will happen if k tends to 0? (this means we don't use regularization)\n",
    "# Q.   Why we need regularixation if we have early stopping?                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a98a2e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3275363445281982\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "xenc_test = F.one_hot(X_test, num_classes = (27*27)).float()\n",
    "logits_test = xenc_test @ W\n",
    "counts_test = logits_test.exp()\n",
    "probs_test = counts_test/ counts_test.sum(1, keepdim = True)\n",
    "loss_3 = -probs_test[torch.arange(test_size),y_test].log().mean()\n",
    "\n",
    "print(loss_3.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb9c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3783e43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ta\n",
      "jojded\n",
      "olkcgaz\n",
      "ma\n",
      "josay\n",
      "wein\n",
      "sa\n",
      "yuiir\n",
      "gtohcassagezvksiania\n",
      "der\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "  out = []\n",
    "  ix = 0\n",
    "  gt = torch.randint(1,27,(1,)).item()  \n",
    "  while True:\n",
    "\n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    ix = (ix*27) + (gt)\n",
    "    out.append(itos[gt])\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27*27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    ix , gt = gt, ix\n",
    "    \n",
    "    if gt == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "62486311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "21ad1e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88776d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EX-3\n",
    "# # What i have to do is:\n",
    "# #     1. use dev-set to tune the strength of smoothing. \n",
    "# #     2. is this mean i have to find the best equation of the regularization to minimize the loss?\n",
    "# #     3. what patterns i see in train and dev set loss as you tune this strength\n",
    "    \n",
    "    \n",
    "# # My plans:\n",
    "    \n",
    "# #     1. First I will see what minimum loss i can get with early stopping\n",
    "# #     2. I i will try different regularization to decrease the loss even further\n",
    "    \n",
    "\n",
    "\n",
    "# # conclusion: I got loss of 2.1 with small k and early stopping technique\n",
    "\n",
    "# let's learn regularization.\n",
    "\n",
    "\n",
    "# #weights haru lai zero ko close ma laaune is one form of label  smoothing, kinaki. if w sabai zero vayo vane.\n",
    "# hamro logits ni zero hunxa. counts = 1 hunxa sabai ko. now probs ta sabai ko same hune vayo ni ta. \n",
    "\n",
    "# so smoothing garne ho vane, sabai zero ko najik ma aaye ramro?\n",
    "# or (sabai W equal vayeni hola ni? hunna jasto lagyo. yes hune raixa copy ma hereko)\n",
    "\n",
    "# what is difference between weights zero banaune and weights equal banaune?\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "#     + (W**2).sum() yo loss ma thapesi ta aba hamro model le parameters haru zero ko najik ma rakhne vayo ni ta.\n",
    "    \n",
    "#     (W**2) garepaxi negative ni positive vayo hai.\n",
    "    \n",
    "#     let's take mean sum ali large huna sakxa\n",
    "    \n",
    "#     + k* (W**2).mean()\n",
    "    \n",
    "#     yo multiplication factor (regularization strength) chai jati tholu vayo teti we punish for W not being close to zero. so ekdam close to zero\n",
    "#     aauxa hola k tholo vayo vane\n",
    "    \n",
    "    \n",
    "#     yo k kati rakhne? experiment garne??\n",
    "    \n",
    "    \n",
    "# wait label smoothing kina garne ra????\n",
    "# hamle +1 gareko theyo ni inf huna nadina, yesma jati thulo + garyo teti label smoothing hunxa. \n",
    "\n",
    "# Jati thulo + garyo is simlar to increasing k.   (wow just wow)\n",
    "\n",
    "\n",
    "\n",
    "# Ok why do we did it?\n",
    "# ->hamro data perfectly accurate ta hunna ni ta, aba mathi data ma j paxi q aaune zero theyo, tara real life ma ta testo hudaina\n",
    "#   kunai na kunai ta hola j paxi q vako. so we add some number to consider those cases.\n",
    "    \n",
    "# Ok i found it just like bias.\n",
    "# Is label smoothing somewhat similar to adding a bias??\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ok learned new things \n",
    "\n",
    "# we got \n",
    "# loss = -probs[torch.arrange(num), ys].log().mean() + 0.01*(W**2).mean() + (i can add term here too, to experiment and make my model better)\n",
    "#                                                                           (if kei vaye loss decrease garna ni payo, or increase garna ni payo)\n",
    "# nice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
