{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0dc71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ************EXERCISE - 02  && 03 ****************\n",
    "\n",
    "\n",
    "Ex-2\n",
    "# what i have to do is\n",
    "# 1. split the dataset randomly into 80% train set, 10% dev set, 10% test set. \n",
    "# 2. train the trigram model only on the training set and evaluate them on dev and test splits.\n",
    "# 3. Evaluate.\n",
    "# Ok \n",
    "\n",
    "\n",
    "# what is dev set?\n",
    "# ok this is used during the training phase, why? -> to see if our model is overfitting or not\n",
    "# if at some point our model starts to perform worst than previous on the dev set, then we can conclude\n",
    "# our model has overfitted, so we can stop here. nice\n",
    "\n",
    "# is stoping is only the choice?\n",
    "\n",
    "# how regularization helps? our model has overfitted mean it already learned too much. i think we should stop\n",
    "# why do regularization and train more?\n",
    "\n",
    "# regularization\n",
    "# -> adding penalty to the loss function to prevent it from overfitting\n",
    "# -> paile nai overfitt vayeni ni feri simple working model ma lane raixa hai regularization le\n",
    "\n",
    "# is regularization better than early stopping? if so why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ed8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Realization\n",
    "\n",
    "i trained my trigram model many times in assignment-1 and i was suprsied why\n",
    "my model is not giving good names eventhough loss was much better.\n",
    "\n",
    "Haha it was due to overfitting. hehe\n",
    "ok so i have to use early stopping technique\n",
    "\n",
    "i will make loop 300 times and use early-stopping technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd78b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plan\n",
    "\n",
    "# I will train the model 2 times.\n",
    "# 1st-->   I will not use early stopping technique\n",
    "# 2nd-->   I will use early stopping technique\n",
    "\n",
    "# And compare which will be better\n",
    "\n",
    "import torch\n",
    "words = open('names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c0b330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11162b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "655d494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  196113\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append((ix1*27)+ix2)\n",
    "    ys.append(ix3)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55a8687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_temp,y_train,y_temp = train_test_split(xs,ys,test_size = 0.2, random_state=42)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp,y_temp, test_size=0.5, random_state = 42)\n",
    "\n",
    "# X_train, y_train ma 80% data xa ani\n",
    "#X_dev ra X_test ma 10 10% data xa ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "888a99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.size())\n",
    "# print(X_dev.size())\n",
    "# print(X_test.size())\n",
    "\n",
    "train_size = len(X_train)\n",
    "dev_size = len(X_dev)\n",
    "test_size = len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1af89585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW i have to create a variable to store the dev_loss every time. if at some point dev_loss(new) > dev_loss(past) \n",
    "#stop the training\n",
    "\n",
    "#Also let's find after how many iteration we did early stopping\n",
    "#Also let's see if doing early stopping make our results better\n",
    "\n",
    "dev_loss = 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc4948b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffa9f938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_loss: 2.1711983680725098\n",
      "loss: 2.1655941009521484\n",
      "dev_loss: 2.170900821685791\n",
      "loss: 2.1652705669403076\n",
      "dev_loss: 2.170604944229126\n",
      "loss: 2.1649491786956787\n",
      "dev_loss: 2.1703109741210938\n",
      "loss: 2.1646296977996826\n",
      "dev_loss: 2.1700191497802734\n",
      "loss: 2.1643121242523193\n",
      "dev_loss: 2.169729471206665\n",
      "loss: 2.163996696472168\n",
      "dev_loss: 2.1694412231445312\n",
      "loss: 2.1636831760406494\n",
      "dev_loss: 2.1691553592681885\n",
      "loss: 2.1633718013763428\n",
      "dev_loss: 2.1688711643218994\n",
      "loss: 2.163062572479248\n",
      "dev_loss: 2.168588876724243\n",
      "loss: 2.162754535675049\n",
      "dev_loss: 2.1683082580566406\n",
      "loss: 2.1624491214752197\n",
      "dev_loss: 2.16802978515625\n",
      "loss: 2.1621451377868652\n",
      "dev_loss: 2.167752742767334\n",
      "loss: 2.1618435382843018\n",
      "dev_loss: 2.16747784614563\n",
      "loss: 2.161543369293213\n",
      "dev_loss: 2.1672046184539795\n",
      "loss: 2.161245107650757\n",
      "dev_loss: 2.166933059692383\n",
      "loss: 2.1609487533569336\n",
      "dev_loss: 2.166663408279419\n",
      "loss: 2.160654067993164\n",
      "dev_loss: 2.166395425796509\n",
      "loss: 2.1603615283966064\n",
      "dev_loss: 2.1661291122436523\n",
      "loss: 2.1600704193115234\n",
      "dev_loss: 2.1658642292022705\n",
      "loss: 2.1597812175750732\n",
      "dev_loss: 2.1656014919281006\n",
      "loss: 2.1594936847686768\n",
      "dev_loss: 2.1653401851654053\n",
      "loss: 2.159207820892334\n",
      "dev_loss: 2.1650805473327637\n",
      "loss: 2.158923864364624\n",
      "dev_loss: 2.164822816848755\n",
      "loss: 2.1586415767669678\n",
      "dev_loss: 2.1645665168762207\n",
      "loss: 2.1583609580993652\n",
      "dev_loss: 2.164311408996582\n",
      "loss: 2.1580820083618164\n",
      "dev_loss: 2.164058208465576\n",
      "loss: 2.1578047275543213\n",
      "dev_loss: 2.163806676864624\n",
      "loss: 2.157528877258301\n",
      "dev_loss: 2.1635570526123047\n",
      "loss: 2.157255172729492\n",
      "dev_loss: 2.163308620452881\n",
      "loss: 2.156982421875\n",
      "dev_loss: 2.1630616188049316\n",
      "loss: 2.1567113399505615\n",
      "dev_loss: 2.162816286087036\n",
      "loss: 2.156442165374756\n",
      "dev_loss: 2.1625723838806152\n",
      "loss: 2.156174421310425\n",
      "dev_loss: 2.162329912185669\n",
      "loss: 2.1559081077575684\n",
      "dev_loss: 2.1620888710021973\n",
      "loss: 2.1556437015533447\n",
      "dev_loss: 2.1618492603302\n",
      "loss: 2.1553804874420166\n",
      "dev_loss: 2.161611318588257\n",
      "loss: 2.155118942260742\n",
      "dev_loss: 2.161374807357788\n",
      "loss: 2.1548585891723633\n",
      "dev_loss: 2.161139726638794\n",
      "loss: 2.154599905014038\n",
      "dev_loss: 2.1609060764312744\n",
      "loss: 2.1543431282043457\n",
      "dev_loss: 2.1606736183166504\n",
      "loss: 2.1540873050689697\n",
      "dev_loss: 2.16044282913208\n",
      "loss: 2.1538331508636475\n",
      "dev_loss: 2.1602132320404053\n",
      "loss: 2.1535801887512207\n",
      "dev_loss: 2.159985065460205\n",
      "loss: 2.1533286571502686\n",
      "dev_loss: 2.1597585678100586\n",
      "loss: 2.153078556060791\n",
      "dev_loss: 2.1595330238342285\n",
      "loss: 2.152830123901367\n",
      "dev_loss: 2.159308671951294\n",
      "loss: 2.152582883834839\n",
      "dev_loss: 2.159085750579834\n",
      "loss: 2.152336835861206\n",
      "dev_loss: 2.1588642597198486\n",
      "loss: 2.152092456817627\n",
      "dev_loss: 2.158644199371338\n",
      "loss: 2.1518492698669434\n",
      "dev_loss: 2.1584255695343018\n",
      "loss: 2.1516075134277344\n",
      "dev_loss: 2.158207893371582\n",
      "loss: 2.151366949081421\n",
      "dev_loss: 2.157991409301758\n",
      "loss: 2.151127815246582\n",
      "dev_loss: 2.157776355743408\n",
      "loss: 2.1508898735046387\n",
      "dev_loss: 2.157562494277954\n",
      "loss: 2.15065336227417\n",
      "dev_loss: 2.1573498249053955\n",
      "loss: 2.1504180431365967\n",
      "dev_loss: 2.1571388244628906\n",
      "loss: 2.150184154510498\n",
      "dev_loss: 2.156928539276123\n",
      "loss: 2.149951219558716\n",
      "dev_loss: 2.156719207763672\n",
      "loss: 2.1497199535369873\n",
      "dev_loss: 2.1565115451812744\n",
      "loss: 2.149489402770996\n",
      "dev_loss: 2.1563050746917725\n",
      "loss: 2.1492605209350586\n",
      "dev_loss: 2.156099557876587\n",
      "loss: 2.1490325927734375\n",
      "dev_loss: 2.155895233154297\n",
      "loss: 2.148805856704712\n",
      "dev_loss: 2.1556923389434814\n",
      "loss: 2.148580551147461\n",
      "dev_loss: 2.1554903984069824\n",
      "loss: 2.1483564376831055\n",
      "dev_loss: 2.1552894115448\n",
      "loss: 2.1481332778930664\n",
      "dev_loss: 2.155089855194092\n",
      "loss: 2.147911310195923\n",
      "dev_loss: 2.1548912525177\n",
      "loss: 2.147690773010254\n",
      "dev_loss: 2.154693841934204\n",
      "loss: 2.1474711894989014\n",
      "dev_loss: 2.1544973850250244\n",
      "loss: 2.1472525596618652\n",
      "dev_loss: 2.1543021202087402\n",
      "loss: 2.147035598754883\n",
      "dev_loss: 2.1541080474853516\n",
      "loss: 2.1468191146850586\n",
      "dev_loss: 2.1539146900177\n",
      "loss: 2.146604061126709\n",
      "dev_loss: 2.1537230014801025\n",
      "loss: 2.146390438079834\n",
      "dev_loss: 2.153531789779663\n",
      "loss: 2.1461775302886963\n",
      "dev_loss: 2.153341770172119\n",
      "loss: 2.145965814590454\n",
      "dev_loss: 2.1531529426574707\n",
      "loss: 2.1457550525665283\n",
      "dev_loss: 2.1529650688171387\n",
      "loss: 2.145545482635498\n",
      "dev_loss: 2.152778148651123\n",
      "loss: 2.145336866378784\n",
      "dev_loss: 2.152592420578003\n",
      "loss: 2.145129442214966\n",
      "dev_loss: 2.15240740776062\n",
      "loss: 2.144922971725464\n",
      "dev_loss: 2.152223825454712\n",
      "loss: 2.1447176933288574\n",
      "dev_loss: 2.152040958404541\n",
      "loss: 2.1445131301879883\n",
      "dev_loss: 2.1518592834472656\n",
      "loss: 2.1443097591400146\n",
      "dev_loss: 2.1516783237457275\n",
      "loss: 2.1441075801849365\n",
      "dev_loss: 2.151498317718506\n",
      "loss: 2.1439061164855957\n",
      "dev_loss: 2.1513192653656006\n",
      "loss: 2.1437058448791504\n",
      "dev_loss: 2.15114164352417\n",
      "loss: 2.1435065269470215\n",
      "dev_loss: 2.1509644985198975\n",
      "loss: 2.143308162689209\n",
      "dev_loss: 2.1507883071899414\n",
      "loss: 2.143110990524292\n",
      "dev_loss: 2.150613307952881\n",
      "loss: 2.1429145336151123\n",
      "dev_loss: 2.1504390239715576\n",
      "loss: 2.142719030380249\n",
      "dev_loss: 2.150265693664551\n",
      "loss: 2.1425247192382812\n",
      "dev_loss: 2.1500933170318604\n",
      "loss: 2.142331123352051\n",
      "dev_loss: 2.1499216556549072\n",
      "loss: 2.1421384811401367\n",
      "dev_loss: 2.1497509479522705\n",
      "loss: 2.141946792602539\n",
      "dev_loss: 2.14958119392395\n",
      "loss: 2.141756057739258\n",
      "dev_loss: 2.1494126319885254\n",
      "loss: 2.141566038131714\n",
      "dev_loss: 2.149244785308838\n",
      "loss: 2.1413772106170654\n",
      "dev_loss: 2.1490776538848877\n",
      "loss: 2.1411895751953125\n",
      "dev_loss: 2.148911476135254\n",
      "loss: 2.1410024166107178\n",
      "dev_loss: 2.1487457752227783\n",
      "loss: 2.1408159732818604\n",
      "dev_loss: 2.1485815048217773\n",
      "loss: 2.1406307220458984\n",
      "dev_loss: 2.1484179496765137\n",
      "loss: 2.140446424484253\n",
      "dev_loss: 2.148254871368408\n",
      "loss: 2.1402628421783447\n",
      "dev_loss: 2.1480932235717773\n",
      "loss: 2.140079975128174\n",
      "dev_loss: 2.1479318141937256\n",
      "loss: 2.1398983001708984\n",
      "dev_loss: 2.1477713584899902\n",
      "loss: 2.1397173404693604\n",
      "dev_loss: 2.1476118564605713\n",
      "loss: 2.1395373344421387\n",
      "dev_loss: 2.1474533081054688\n",
      "loss: 2.1393580436706543\n",
      "dev_loss: 2.1472954750061035\n",
      "loss: 2.1391797065734863\n",
      "dev_loss: 2.1471381187438965\n",
      "loss: 2.1390020847320557\n",
      "dev_loss: 2.146981954574585\n",
      "loss: 2.138824939727783\n",
      "dev_loss: 2.1468262672424316\n",
      "loss: 2.1386489868164062\n",
      "dev_loss: 2.1466715335845947\n",
      "loss: 2.1384739875793457\n",
      "dev_loss: 2.146517515182495\n",
      "loss: 2.1382997035980225\n",
      "dev_loss: 2.146364450454712\n",
      "loss: 2.1381261348724365\n",
      "dev_loss: 2.146211862564087\n",
      "loss: 2.137953281402588\n",
      "dev_loss: 2.1460602283477783\n",
      "loss: 2.1377813816070557\n",
      "dev_loss: 2.145909309387207\n",
      "loss: 2.13761043548584\n",
      "dev_loss: 2.145759105682373\n",
      "loss: 2.137439727783203\n",
      "dev_loss: 2.1456093788146973\n",
      "loss: 2.137270212173462\n",
      "dev_loss: 2.145460844039917\n",
      "loss: 2.137101173400879\n",
      "dev_loss: 2.145312786102295\n",
      "loss: 2.1369333267211914\n",
      "dev_loss: 2.14516544342041\n",
      "loss: 2.136765718460083\n",
      "dev_loss: 2.1450188159942627\n",
      "loss: 2.13659930229187\n",
      "dev_loss: 2.1448731422424316\n",
      "loss: 2.1364336013793945\n",
      "dev_loss: 2.144728183746338\n",
      "loss: 2.136268377304077\n",
      "dev_loss: 2.1445837020874023\n",
      "loss: 2.136104106903076\n",
      "dev_loss: 2.144439935684204\n",
      "loss: 2.1359403133392334\n",
      "dev_loss: 2.144296884536743\n",
      "loss: 2.135777473449707\n",
      "dev_loss: 2.1441545486450195\n",
      "loss: 2.135615110397339\n",
      "dev_loss: 2.144012689590454\n",
      "loss: 2.135453939437866\n",
      "dev_loss: 2.143872022628784\n",
      "loss: 2.1352932453155518\n",
      "dev_loss: 2.1437318325042725\n",
      "loss: 2.1351330280303955\n",
      "dev_loss: 2.143592119216919\n",
      "loss: 2.1349737644195557\n",
      "dev_loss: 2.143453359603882\n",
      "loss: 2.134815216064453\n",
      "dev_loss: 2.143315076828003\n",
      "loss: 2.134657144546509\n",
      "dev_loss: 2.1431772708892822\n",
      "loss: 2.1344997882843018\n",
      "dev_loss: 2.143040418624878\n",
      "loss: 2.134343385696411\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m#backward pass:\u001b[39;00m\n\u001b[0;32m     44\u001b[0m W\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;66;03m# paile sabaile gradient lai zero gareko\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m#magic hehe\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#update\u001b[39;00m\n\u001b[0;32m     49\u001b[0m W\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m80\u001b[39m\u001b[38;5;241m*\u001b[39mW\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for k in range(1000):\n",
    "    \n",
    "    \n",
    "    #forward pass for train-set\n",
    "    \n",
    "    xenc = F.one_hot(X_train, num_classes = (27*27)).float() #input to the network: one-hot encoding\n",
    "    logits = xenc@W  #log-counts predict gareko hai\n",
    "    counts = logits.exp()  #coutns, equivalent to N\n",
    "    probs = counts/counts.sum(1,keepdim = True) #probabilities for next character\n",
    "    \n",
    "    loss = -probs[torch.arange(train_size),y_train].log().mean()  # (removing this, i suspect this is regularization) + 0.01 * (W**2).mean()  # i haven't understood this second part\n",
    "    \n",
    "    \n",
    "    #forward pass for dev-set\n",
    "    xenc_dev = F.one_hot(X_dev, num_classes = (27*27)).float()\n",
    "    logits_dev = xenc_dev @ W\n",
    "    counts_dev = logits_dev.exp()\n",
    "    probs_dev = counts_dev/ counts_dev.sum(1, keepdim = True)\n",
    "    loss_2 = -probs_dev[torch.arange(dev_size),y_dev].log().mean()\n",
    "    \n",
    "    print(f\"dev_loss: {loss_2.item()}\")\n",
    "    \n",
    "    if(dev_loss < loss_2):\n",
    "        print(f\"early stopping done, stoped at: {k}\")\n",
    "        break\n",
    "    else:\n",
    "        dev_loss = loss_2\n",
    "    #maile bujeko yo loss chai,\n",
    "#     probs[torch.arrange(num),ys] this gives probability predcited by our model for ground truth. if it is less than \n",
    "#     100% model we give it a loss i.e -loglikelihood now model updates it's parameters.\n",
    "    \n",
    "#     Q. I am super-amazed how this loss worked? \n",
    "#     --> because we are not always sure what letters comes after pairs in real names. any letter can come. \n",
    "#         but some are more likely.\n",
    "#     --> i think our model will twiks our W such that it gives more probabilty for more likely words based on training set\n",
    "    \n",
    "#     Q. Still confused how loss will converged? I suspect it will not. (noob me, think later)\n",
    "#     Q. \n",
    "\n",
    "     \n",
    "    print(f\"loss: {loss.item()}\")\n",
    "    #backward pass:\n",
    "    \n",
    "    W.grad = None # paile sabaile gradient lai zero gareko\n",
    "    loss.backward()  #magic hehe\n",
    "    \n",
    "    #update\n",
    "    \n",
    "    W.data += -80*W.grad   #remember to increment opp to direction of gradient, otherwise face the consequences)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nice, we stopped early at 50 iteration with loss 2.455\n",
    "#update, many iterations with loss 2.13\n",
    "# wow nice we stopped the overfitting here, Hurrehy!!!!!!\n",
    "\n",
    "\n",
    "# let's see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a98a2e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.160979986190796\n"
     ]
    }
   ],
   "source": [
    "#here loss is very good. let's test in test set\n",
    "\n",
    "xenc_test = F.one_hot(X_test, num_classes = (27*27)).float()\n",
    "logits_test = xenc_test @ W\n",
    "counts_test = logits_test.exp()\n",
    "probs_test = counts_test/ counts_test.sum(1, keepdim = True)\n",
    "loss_3 = -probs_test[torch.arange(test_size),y_test].log().mean()\n",
    "\n",
    "print(loss_3.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eeb9c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wow loss on test set is also very good.\n",
    "\n",
    "#which means our model is performing good.\n",
    "\n",
    "#let's verify it by the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3783e43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da\n",
      "monde\n",
      "milyasid\n",
      "na\n",
      "yus\n",
      "ey\n",
      "welin\n",
      "reshir\n",
      "qtonian\n",
      "jayel\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "  out = []\n",
    "  ix = 0\n",
    "  gt = torch.randint(1,27,(1,)).item()  \n",
    "  while True:\n",
    "\n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    ix = (ix*27) + (gt)\n",
    "    out.append(itos[gt])\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27*27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    ix , gt = gt, ix\n",
    "    \n",
    "    if gt == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "62486311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # OK i don't see any difference. names still sucks\n",
    "\n",
    "\n",
    "# Do i have to use regularizations or what?\n",
    "# it is giving shitty names?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "21ad1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wtqkwumdhntauvrfdleniajkdbyainrwibwlasnjywnfyla    wtf is this name???\n",
    "\n",
    "# correction, my forward pass for devset was incorrect\n",
    "\n",
    "#update: Now names are good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88776d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "EX-3\n",
    "What i have to do is:\n",
    "    1. use dev-set to tune the strength of smoothing. \n",
    "    2. is this mean i have to find the best equation of the regularization to minimize the loss?\n",
    "    3. what patterns i see in train and dev set loss as you tune this strength\n",
    "    \n",
    "    \n",
    "My plans:\n",
    "    \n",
    "    1. First I will see what minimum loss i can get with early stopping\n",
    "    2. I i will try different regularization to decrease the loss even further\n",
    "    \n",
    "\n",
    "\n",
    "minimum_loss_with_early_stopping:    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
