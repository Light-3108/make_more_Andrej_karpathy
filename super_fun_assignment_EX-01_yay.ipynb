{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0dc71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Andrej karpathy -- Assignments \n",
    "\n",
    "#Ex-1\n",
    "What i have to do is:\n",
    "    \n",
    "#training a trigram model, 2 char as input to predict 3rd one. \n",
    "# I am using both counting and neural network\n",
    "# I have to evaulate the loss\n",
    "# And answer the question, did it improve over the bigram model? (he made video about the bigram model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd78b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I have to make 2d matrix of size (27*27)(rows) *(27)(columns)\n",
    "# because .a,.b ..... aa,ab ........, za,zb..... is (27*27)\n",
    "# and output can be any 27 letters\n",
    "\n",
    "#ok let's start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e5a3f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cf715f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma',\n",
       " 'olivia',\n",
       " 'ava',\n",
       " 'isabella',\n",
       " 'sophia',\n",
       " 'charlotte',\n",
       " 'mia',\n",
       " 'amelia',\n",
       " 'harper',\n",
       " 'evelyn']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c72dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43bfedc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27*27,27),dtype = torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c0b330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b41aeae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    N[(27*ix1)+(ix2),ix3] +=1\n",
    "# mx = -1\n",
    "# for i in range(27*27):\n",
    "#     for j in range(27):\n",
    "#         mx = max(mx,N[i,j])\n",
    "# print(mx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88519a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's visualize, i know it's not a good considering how big the picture would be, hehehe\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# plt.figure(figsize=(16*27,16))\n",
    "# plt.imshow(N, cmap='Blues')\n",
    "# for i in range(27*27):\n",
    "#     for j in range(27):\n",
    "        \n",
    "#         first_char = i//27\n",
    "#         second_char = i%27\n",
    "        \n",
    "#         chstr = itos[first_char] + itos[second_char] + '-'+ itos[j]\n",
    "#         plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color='gray')\n",
    "#         plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color='gray')\n",
    "# plt.axis('off');\n",
    "\n",
    "# i don't know why it is taking so long, let's do it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "202c6cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0, 169,   0,   0,   0, 253,   0,   0,   9,  41,   1,   0,  85,   0,\n",
      "          0,  77,   0,   0, 646,   0,   0,  21,   0,   0,   0,   4,   0],\n",
      "       dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(N[2])  # .b paxi kun char ko count badi xa?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8586e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([27])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "P = (N+1).float()\n",
    "P /= P.sum(1,keepdim = True) #729 * 27 / 1 * 27  broadcast able\n",
    "print(P[0].size())\n",
    "print(P[0].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3af45996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suuwjde\n",
      "vianasid\n",
      "gus\n",
      "elay\n",
      "zain\n",
      "na\n",
      "devin\n",
      "itoper\n",
      "grayel\n",
      "sus\n"
     ]
    }
   ],
   "source": [
    "#nice, 1 nai aauna parne ho\n",
    "\n",
    "\n",
    "#let's generate a name now. hehe\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) #using this generator\n",
    "\n",
    "for i in range(10): #let's generate 10 names\n",
    "\n",
    "  out = []\n",
    "  ix = 0 # zero index ma hereko._ kun letter chain first ma suru hunxa\n",
    "  #first i have to input two letters . and any 26 char randomly:\n",
    "\n",
    "  gt = torch.randint(0,27,(1,)).item()  \n",
    "    \n",
    "  while True:\n",
    "    # p = N[ix].float()  #sab ko number nikalyo ix th char paxi k aauxa vanera\n",
    "    # p = p / p.sum() # probabilty ma lageko\n",
    "    \n",
    "    ix = (ix*27) + (gt)   # giving 2 char, because this is trigram model\n",
    "    \n",
    "    \n",
    "    p = P[ix]\n",
    "    out.append(itos[gt])\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    #sample 1 leko. jun chain ix th char ko paxadi aaune probability badi xa, tei char ko index ho yo new ix\n",
    "    ix , gt = gt, ix\n",
    "\n",
    "    if gt == 0:  #end vaye ta break garnu paryo ni ta\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a2b7e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wow i got the name rock. haha. \n",
    "# novin, could be a good AI name.\n",
    "# haree, it's literally my brother name\n",
    "# wow, nice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78bba77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing above result, i think trigram is better than bigram, but not sure\n",
    "# I have some flaw in my implemention.  i guess randomly the first char. but i could choose those charters which are likely to come first\n",
    "# MY way would generate a name even more unique so i am ok with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85e7c85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if we consider all letters are equally likely then 1/27 ~ 4%\n",
    "# so any probabilty > 4% is better\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91076afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: log_likelihood=tensor(-410414.9688)\n",
      "negative log likelihood: nll=tensor(410414.9688)\n",
      "Avg negative-log likelihood: nll/n=tensor(2.0927)\n"
     ]
    }
   ],
   "source": [
    "#finding loss\n",
    "\n",
    "\n",
    "log_likelihood = 0.0\n",
    "\n",
    "n = 0\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    prob = P[(ix1*27) + ix2, ix3]\n",
    "    log_likelihood += torch.log(prob)\n",
    "    n+=1\n",
    "    # print(f\"{ch1}{ch2}:  {prob:.4f}\")\n",
    "\n",
    "print(f\"loss: {log_likelihood=}\")\n",
    "nll  = - log_likelihood\n",
    "print(f\"negative log likelihood: {nll=}\")\n",
    "print(f\"Avg negative-log likelihood: {nll/n=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c4fde6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg negative log likelihood of our trigram model is less than bigram model hence this is better\n",
    "# i still don't understand how loss works in this case?\n",
    "# is predicting next word based on more past words makes our prediction better?\n",
    "# How this loss is finding this model is better?\n",
    "# i am still confused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db10204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0f67f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "02a19344",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now let's use neural network let's go......\n",
    "#creating training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d530b150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".e m\n",
      "em m\n",
      "mm a\n",
      "ma .\n",
      "tensor([  5, 148, 364, 352])\n",
      "tensor([13, 13,  1,  0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "xs, ys = [], []\n",
    "for w in words[:1]:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append((ix1*27)+ix2)\n",
    "    ys.append(ix3)\n",
    "    print(f\"{ch1+ch2} {ch3}\")\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print(xs)\n",
    "print(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8e0cbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encodeing is needed. omg vector of size (27*27) is this efficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c11162b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 729])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes = (27*27)).float()\n",
    "xenc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a4aa229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6cfa608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xenc.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0241ca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.randn((27*27,27))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ca1ae1f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.8599)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([729])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print((xenc@W)[2,13])\n",
    "(W[:,13]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7ca28777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.8599)\n"
     ]
    }
   ],
   "source": [
    "#same\n",
    "print(xenc[2]@W[:,13])  # still"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b006775e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok neural network architecture looks like this\n",
    "# 27*27 neurons in the input ---->  27 outputs..    NO hidden layers\n",
    "\n",
    "# now we have to introduce loss to change the weights.\n",
    "# multiple examples line, label vai halxa ani gradient line ani parameters update garne.\n",
    "\n",
    "# kaile a paxi b aaule kaile a paxi c aauxla kaile feri a paxi b aaula. tara hamro large examples ko loss nikalera\n",
    "# parameters update garda, milla. a paxi b aaune probabilty ali badi hola tei ho\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "42f44453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = xenc @ W  #log counts\n",
    "\n",
    "#yo logits ta ouput ho hai? yeslai softmax garda hola\n",
    "counts = logits.exp() # equivalent to N\n",
    "probs = counts/counts.sum(1,keepdim = True)\n",
    "\n",
    "probs[0].sum()\n",
    "\n",
    "#soft-max use gareko hai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea027775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------\n",
      "bigram example 1: .em (indexes 5,13)\n",
      "input to the neural net: 5\n",
      "output probabilities from the neural net: tensor([0.0777, 0.0145, 0.0035, 0.0203, 0.0753, 0.0746, 0.0145, 0.0253, 0.0080,\n",
      "        0.0057, 0.0010, 0.0357, 0.0516, 0.0286, 0.0225, 0.0042, 0.0639, 0.0066,\n",
      "        0.0455, 0.0768, 0.0063, 0.0490, 0.0575, 0.0340, 0.0058, 0.1690, 0.0224])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.02863534353673458\n",
      "log likelihood: -3.5531134605407715\n",
      "negative log likelihood: 3.5531134605407715\n",
      "--------\n",
      "bigram example 2: emm (indexes 148,13)\n",
      "input to the neural net: 148\n",
      "output probabilities from the neural net: tensor([0.0094, 0.0215, 0.0240, 0.0147, 0.0182, 0.0547, 0.0531, 0.0396, 0.0308,\n",
      "        0.0081, 0.0966, 0.0236, 0.0237, 0.0233, 0.0143, 0.0625, 0.0812, 0.0274,\n",
      "        0.0160, 0.0347, 0.0249, 0.0064, 0.0426, 0.0391, 0.1395, 0.0622, 0.0080])\n",
      "label (actual next character): 13\n",
      "probability assigned by the net to the the correct character: 0.023344168439507484\n",
      "log likelihood: -3.7574081420898438\n",
      "negative log likelihood: 3.7574081420898438\n",
      "--------\n",
      "bigram example 3: mma (indexes 364,1)\n",
      "input to the neural net: 364\n",
      "output probabilities from the neural net: tensor([0.0427, 0.0255, 0.0087, 0.0455, 0.1134, 0.0790, 0.0156, 0.0226, 0.0374,\n",
      "        0.0017, 0.0106, 0.0291, 0.0179, 0.0099, 0.1977, 0.0143, 0.0646, 0.0090,\n",
      "        0.0651, 0.0404, 0.0220, 0.0078, 0.0110, 0.0287, 0.0100, 0.0545, 0.0154])\n",
      "label (actual next character): 1\n",
      "probability assigned by the net to the the correct character: 0.025490131229162216\n",
      "log likelihood: -3.669463872909546\n",
      "negative log likelihood: 3.669463872909546\n",
      "--------\n",
      "bigram example 4: ma. (indexes 352,0)\n",
      "input to the neural net: 352\n",
      "output probabilities from the neural net: tensor([0.1033, 0.0022, 0.0068, 0.1559, 0.0257, 0.0224, 0.0405, 0.0048, 0.0111,\n",
      "        0.0698, 0.1174, 0.0139, 0.0451, 0.0101, 0.0392, 0.0268, 0.0285, 0.0158,\n",
      "        0.0503, 0.0103, 0.0384, 0.0273, 0.0092, 0.0116, 0.0200, 0.0044, 0.0892])\n",
      "label (actual next character): 0\n",
      "probability assigned by the net to the the correct character: 0.10328157991170883\n",
      "log likelihood: -2.270296335220337\n",
      "negative log likelihood: 2.270296335220337\n",
      "=========\n",
      "average negative log likelihood, i.e. loss = 3.312570333480835\n"
     ]
    }
   ],
   "source": [
    "nlls = torch.zeros(4)\n",
    "for i in range(4):\n",
    "    \n",
    "  # i-th bigram:\n",
    "  x = xs[i].item() # input character index\n",
    "  y = ys[i].item() # label character index\n",
    "  \n",
    "  ch1 = (x//27)\n",
    "  ch2 = x%27\n",
    "  print('--------')\n",
    "  print(f'bigram example {i+1}: {itos[ch1]+itos[ch2]}{itos[y]} (indexes {x},{y})')\n",
    "  print('input to the neural net:', x)\n",
    "  print('output probabilities from the neural net:', probs[i])\n",
    "  print('label (actual next character):', y)\n",
    "  p = probs[i, y]\n",
    "  print('probability assigned by the net to the the correct character:', p.item())\n",
    "  logp = torch.log(p)\n",
    "  print('log likelihood:', logp.item())\n",
    "  nll = -logp\n",
    "  print('negative log likelihood:', nll.item())\n",
    "  nlls[i] = nll\n",
    "\n",
    "\n",
    "print('=========')\n",
    "print('average negative log likelihood, i.e. loss =', nlls.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8b2ac31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss ramro xaina, let's train, let's goooo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "21407f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (this may be wrong, I just gussed based on  what i have understood)\n",
    "\n",
    "# Q. what is mean by training here?\n",
    "# -> it is updating our W matirx.\n",
    "# Q. how you update our W matrix here?\n",
    "# -> traing example banako xa ni ta. so hamle forward pass garxum. our model perform based on it's parameter\n",
    "#    now we find the loss. (using our training data as label, ofcourse). \n",
    "#    sabai data ma ekai choti garne hola hai? train?\n",
    "#    ani feri gareko garekai kina garne ra?\n",
    "#    gradient descent vako xani ta so. loop gareko garekai garesi local minima ma janxa ni ta.\n",
    "\n",
    "# nice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "84fdbda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent let's goooooooooooooooooooooooo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "655d494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  196113\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append((ix1*27)+ix2)\n",
    "    ys.append(ix3)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ffa9f938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.409698009490967\n",
      "2.4073662757873535\n",
      "2.405068874359131\n",
      "2.4028048515319824\n",
      "2.400573492050171\n",
      "2.398374080657959\n",
      "2.3962056636810303\n",
      "2.3940680027008057\n",
      "2.391960382461548\n",
      "2.3898818492889404\n",
      "2.3878324031829834\n",
      "2.3858110904693604\n",
      "2.383816719055176\n",
      "2.381850004196167\n",
      "2.3799092769622803\n",
      "2.3779945373535156\n",
      "2.376105546951294\n",
      "2.3742408752441406\n",
      "2.372401237487793\n",
      "2.3705852031707764\n",
      "2.36879301071167\n",
      "2.367023468017578\n",
      "2.365276575088501\n",
      "2.3635518550872803\n",
      "2.361848831176758\n",
      "2.3601672649383545\n",
      "2.358506441116333\n",
      "2.3568663597106934\n",
      "2.355246067047119\n",
      "2.3536458015441895\n",
      "2.352065324783325\n",
      "2.35050368309021\n",
      "2.3489606380462646\n",
      "2.3474361896514893\n",
      "2.3459293842315674\n",
      "2.3444411754608154\n",
      "2.3429694175720215\n",
      "2.341515302658081\n",
      "2.340078115463257\n",
      "2.3386569023132324\n",
      "2.337252378463745\n",
      "2.3358638286590576\n",
      "2.3344905376434326\n",
      "2.3331332206726074\n",
      "2.3317906856536865\n",
      "2.330463171005249\n",
      "2.3291501998901367\n",
      "2.3278512954711914\n",
      "2.3265669345855713\n",
      "2.325296640396118\n",
      "2.3240394592285156\n",
      "2.322796106338501\n",
      "2.321565628051758\n",
      "2.3203482627868652\n",
      "2.3191440105438232\n",
      "2.3179519176483154\n",
      "2.3167724609375\n",
      "2.3156051635742188\n",
      "2.3144495487213135\n",
      "2.3133058547973633\n",
      "2.312174081802368\n",
      "2.3110532760620117\n",
      "2.309943914413452\n",
      "2.3088455200195312\n",
      "2.307758092880249\n",
      "2.3066811561584473\n",
      "2.305614709854126\n",
      "2.304558753967285\n",
      "2.303513288497925\n",
      "2.3024775981903076\n",
      "2.3014519214630127\n",
      "2.300436019897461\n",
      "2.299429416656494\n",
      "2.2984325885772705\n",
      "2.297445058822632\n",
      "2.296466827392578\n",
      "2.295497179031372\n",
      "2.29453706741333\n",
      "2.2935853004455566\n",
      "2.29264235496521\n",
      "2.29170823097229\n",
      "2.2907819747924805\n",
      "2.2898643016815186\n",
      "2.288954973220825\n",
      "2.288053512573242\n",
      "2.2871599197387695\n",
      "2.2862744331359863\n",
      "2.2853963375091553\n",
      "2.2845263481140137\n",
      "2.283663749694824\n",
      "2.2828080654144287\n",
      "2.2819600105285645\n",
      "2.2811193466186523\n",
      "2.280285596847534\n",
      "2.27945876121521\n",
      "2.278639316558838\n",
      "2.2778265476226807\n",
      "2.2770204544067383\n",
      "2.2762207984924316\n",
      "2.275428295135498\n"
     ]
    }
   ],
   "source": [
    "for k in range(100):\n",
    "    \n",
    "    \n",
    "    #forward pass\n",
    "    \n",
    "    xenc = F.one_hot(xs, num_classes = (27*27)).float() #input to the network: one-hot encoding\n",
    "    logits = xenc@W  #log-counts predict gareko hai\n",
    "    counts = logits.exp()  #coutns, equivalent to N\n",
    "    probs = counts/counts.sum(1,keepdim = True) #probabilities for next character\n",
    "    \n",
    "    loss = -probs[torch.arange(num),ys].log().mean()  + 0.01 * (W**2).mean()  # i haven't understood this second part\n",
    "    \n",
    "    #maile bujeko yo loss chai,\n",
    "#     probs[torch.arrange(num),ys] this gives probability predcited by our model for ground truth. if it is less than \n",
    "#     100% model we give it a loss i.e -loglikelihood now model updates it's parameters.\n",
    "    \n",
    "#     Q. I am super-amazed how this loss worked? \n",
    "#     --> because we are not always sure what letters comes after pairs in real names. any letter can come. \n",
    "#         but some are more likely.\n",
    "#     --> i think our model will twiks our W such that it gives more probabilty for more likely words based on training set\n",
    "    \n",
    "#     Q. Still confused how loss will converged? I suspect it will not. (noob me, think later)\n",
    "#     Q. \n",
    "\n",
    "     \n",
    "    print(loss.item())\n",
    "    #backward pass:\n",
    "    \n",
    "    W.grad = None # paile sabaile gradient lai zero gareko\n",
    "    loss.backward()  #magic hehe\n",
    "    \n",
    "    #update\n",
    "    \n",
    "    W.data += -50*W.grad   #remember to increment opp to direction of gradient, otherwise face the consequences)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wow this is giving much better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3783e43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el\n",
      "kojde\n",
      "milyaqah\n",
      "gus\n",
      "elay\n",
      "lein\n",
      "gandi\n",
      "xrvtonian\n",
      "caree\n",
      "aviahnia\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "  out = []\n",
    "  ix = 0\n",
    "  gt = torch.randint(1,27,(1,)).item()  \n",
    "  while True:\n",
    "\n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    ix = (ix*27) + (gt)\n",
    "    out.append(itos[gt])\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27*27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    ix , gt = gt, ix\n",
    "    \n",
    "    if gt == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62486311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different name came, i don't know why?\n",
    "# some names doesn't seems right\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
