{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0dc71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ************EXERCISE - 02  && 03 ****************\n",
    "\n",
    " ** there are many changes from exercise - 2 **\n",
    "    \n",
    "    \n",
    "Ex-2\n",
    "# what i have to do is\n",
    "# 1. split the dataset randomly into 80% train set, 10% dev set, 10% test set. \n",
    "# 2. train the trigram model only on the training set and evaluate them on dev and test splits.\n",
    "# 3. Evaluate.\n",
    "# Ok \n",
    "\n",
    "\n",
    "# what is dev set?\n",
    "# ok this is used during the training phase, why? -> to see if our model is overfitting or not\n",
    "# if at some point our model starts to perform worst than previous on the dev set, then we can conclude\n",
    "# our model has overfitted, so we can stop here. nice\n",
    "\n",
    "# is stoping is only the choice?\n",
    "\n",
    "# how regularization helps? our model has overfitted mean it already learned too much. i think we should stop\n",
    "# why do regularization and train more?\n",
    "\n",
    "# regularization\n",
    "# -> adding penalty to the loss function to prevent it from overfitting\n",
    "# -> paile nai overfitt vayeni ni feri simple working model ma lane raixa hai regularization le\n",
    "\n",
    "# is regularization better than early stopping? if so why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0ed8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Realization\n",
    "\n",
    "i trained my trigram model many times in assignment-1 and i was suprsied why\n",
    "my model is not giving good names eventhough loss was much better.\n",
    "\n",
    "Haha it was due to overfitting. hehe\n",
    "ok so i have to use early stopping technique\n",
    "\n",
    "i will make loop 300 times and use early-stopping technique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cd78b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plan\n",
    "\n",
    "# I will train the model 2 times.\n",
    "# 1st-->   I will not use early stopping technique\n",
    "# 2nd-->   I will use early stopping technique\n",
    "\n",
    "# And compare which will be better\n",
    "\n",
    "import torch\n",
    "words = open('names.txt','r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c0b330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11162b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "655d494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  196113\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "\n",
    "xs, ys = [], []\n",
    "for w in words:\n",
    "  chs = ['.'] + list(w) + ['.']\n",
    "  for ch1,ch2,ch3 in zip(chs,chs[1:],chs[2:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    ix3 = stoi[ch3]\n",
    "    xs.append((ix1*27)+ix2)\n",
    "    ys.append(ix3)\n",
    "    \n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('number of examples: ', num)\n",
    "\n",
    "# initialize the 'network'\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W = torch.randn((27*27, 27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55a8687c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_temp,y_train,y_temp = train_test_split(xs,ys,test_size = 0.2, random_state=42)\n",
    "X_dev, X_test, y_dev, y_test = train_test_split(X_temp,y_temp, test_size=0.5, random_state = 42)\n",
    "\n",
    "# X_train, y_train ma 80% data xa ani\n",
    "#X_dev ra X_test ma 10 10% data xa ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "888a99ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(X_train.size())\n",
    "# print(X_dev.size())\n",
    "# print(X_test.size())\n",
    "\n",
    "train_size = len(X_train)\n",
    "dev_size = len(X_dev)\n",
    "test_size = len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1af89585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW i have to create a variable to store the dev_loss every time. if at some point dev_loss(new) > dev_loss(past) \n",
    "#stop the training\n",
    "\n",
    "#Also let's find after how many iteration we did early stopping\n",
    "#Also let's see if doing early stopping make our results better\n",
    "\n",
    "dev_loss = 1e10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc4948b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ffa9f938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev_loss: 2.5705671310424805\n",
      "loss: 3.0613694190979004\n",
      "dev_loss: 2.5888638496398926\n",
      "early stopping done, stoped at: 1\n"
     ]
    }
   ],
   "source": [
    "for k in range(20):\n",
    "    \n",
    "    \n",
    "    #forward pass for train-set\n",
    "    \n",
    "    xenc = F.one_hot(X_train, num_classes = (27*27)).float() #input to the network: one-hot encoding\n",
    "    logits = xenc@W  #log-counts predict gareko hai\n",
    "    counts = logits.exp()  #coutns, equivalent to N\n",
    "    probs = counts/counts.sum(1,keepdim = True) #probabilities for next character\n",
    "    \n",
    "    loss = -probs[torch.arange(train_size),y_train].log().mean() + 0.01 * (W**2).mean()  # i haven't understood this second part\n",
    "    \n",
    "    \n",
    "    #forward pass for dev-set\n",
    "    xenc_dev = F.one_hot(X_dev, num_classes = (27*27)).float()\n",
    "    logits_dev = xenc_dev @ W\n",
    "    counts_dev = logits_dev.exp()\n",
    "    probs_dev = counts_dev/ counts_dev.sum(1, keepdim = True)\n",
    "    loss_2 = -probs_dev[torch.arange(dev_size),y_dev].log().mean()\n",
    "    \n",
    "    print(f\"dev_loss: {loss_2.item()}\")\n",
    "    \n",
    "    if(dev_loss < loss_2):\n",
    "        print(f\"early stopping done, stoped at: {k}\")\n",
    "        break\n",
    "    else:\n",
    "        dev_loss = loss_2\n",
    "    #maile bujeko yo loss chai,\n",
    "#     probs[torch.arrange(num),ys] this gives probability predcited by our model for ground truth. if it is less than \n",
    "#     100% model we give it a loss i.e -loglikelihood now model updates it's parameters.\n",
    "    \n",
    "#     Q. I am super-amazed how this loss worked? \n",
    "#     --> because we are not always sure what letters comes after pairs in real names. any letter can come. \n",
    "#         but some are more likely.\n",
    "#     --> i think our model will twiks our W such that it gives more probabilty for more likely words based on training set\n",
    "    \n",
    "#     Q. Still confused how loss will converged? I suspect it will not. (noob me, think later)\n",
    "#     Q. \n",
    "\n",
    "     \n",
    "    print(f\"loss: {loss.item()}\")\n",
    "    #backward pass:\n",
    "    \n",
    "    W.grad = None # paile sabaile gradient lai zero gareko\n",
    "    loss.backward()  #magic hehe\n",
    "    \n",
    "    #update\n",
    "    \n",
    "    W.data += -80*W.grad   #remember to increment opp to direction of gradient, otherwise face the consequences)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "daba6bc1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 12) (1102419117.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[27], line 12\u001b[1;36m\u001b[0m\n\u001b[1;33m    what i don't know\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 12)\n"
     ]
    }
   ],
   "source": [
    "# i noticed if i change 0.01 to 0.0001 network learns slower\n",
    "# ok this is not the case i think, does this have any advantages?\n",
    "\n",
    "# if multiplicating factor(K)is too big, faster overfitting which helps to generalize the data?\n",
    "#  if we decrease K, penalty term becomes less significant. so model may overfitt. (meaning good on train, bad on test)\n",
    "#  what about small k with early stopping?\n",
    "\n",
    "# ---> because we are using early stopping to prevent model from overfitting and using small k to get high accuracy?\n",
    "\n",
    "# is this right?\n",
    "\n",
    "# what i don't know\n",
    "# Q.   What will happen if k tends to 0? (this means we don't use regularization)\n",
    "# Q.   Why we need regularixation if we have early stopping?                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a98a2e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.160979986190796\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "xenc_test = F.one_hot(X_test, num_classes = (27*27)).float()\n",
    "logits_test = xenc_test @ W\n",
    "counts_test = logits_test.exp()\n",
    "probs_test = counts_test/ counts_test.sum(1, keepdim = True)\n",
    "loss_3 = -probs_test[torch.arange(test_size),y_test].log().mean()\n",
    "\n",
    "print(loss_3.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8eeb9c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3783e43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da\n",
      "monde\n",
      "milyasid\n",
      "na\n",
      "yus\n",
      "ey\n",
      "welin\n",
      "reshir\n",
      "qtonian\n",
      "jayel\n"
     ]
    }
   ],
   "source": [
    "# finally, sample from the 'neural net' model\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "  out = []\n",
    "  ix = 0\n",
    "  gt = torch.randint(1,27,(1,)).item()  \n",
    "  while True:\n",
    "\n",
    "    # ----------\n",
    "    # BEFORE:\n",
    "    #p = P[ix]\n",
    "    # ----------\n",
    "    # NOW:\n",
    "    ix = (ix*27) + (gt)\n",
    "    out.append(itos[gt])\n",
    "    xenc = F.one_hot(torch.tensor([ix]), num_classes=27*27).float()\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    # ----------\n",
    "\n",
    "    ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "    ix , gt = gt, ix\n",
    "    \n",
    "    if gt == 0:\n",
    "      break\n",
    "  print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "62486311",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "21ad1e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88776d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # EX-3\n",
    "# # What i have to do is:\n",
    "# #     1. use dev-set to tune the strength of smoothing. \n",
    "# #     2. is this mean i have to find the best equation of the regularization to minimize the loss?\n",
    "# #     3. what patterns i see in train and dev set loss as you tune this strength\n",
    "    \n",
    "    \n",
    "# # My plans:\n",
    "    \n",
    "# #     1. First I will see what minimum loss i can get with early stopping\n",
    "# #     2. I i will try different regularization to decrease the loss even further\n",
    "    \n",
    "\n",
    "\n",
    "# # conclusion: I got loss of 2.1 with small k and early stopping technique\n",
    "\n",
    "# let's learn regularization.\n",
    "\n",
    "\n",
    "# #weights haru lai zero ko close ma laaune is one form of label  smoothing, kinaki. if w sabai zero vayo vane.\n",
    "# hamro logits ni zero hunxa. counts = 1 hunxa sabai ko. now probs ta sabai ko same hune vayo ni ta. \n",
    "\n",
    "# so smoothing garne ho vane, sabai zero ko najik ma aaye ramro?\n",
    "# or (sabai W equal vayeni hola ni? hunna jasto lagyo. yes hune raixa copy ma hereko)\n",
    "\n",
    "# what is difference between weights zero banaune and weights equal banaune?\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "#     + (W**2).sum() yo loss ma thapesi ta aba hamro model le parameters haru zero ko najik ma rakhne vayo ni ta.\n",
    "    \n",
    "#     (W**2) garepaxi negative ni positive vayo hai.\n",
    "    \n",
    "#     let's take mean sum ali large huna sakxa\n",
    "    \n",
    "#     + k* (W**2).mean()\n",
    "    \n",
    "#     yo multiplication factor (regularization strength) chai jati tholu vayo teti we punish for W not being close to zero. so ekdam close to zero\n",
    "#     aauxa hola k tholo vayo vane\n",
    "    \n",
    "    \n",
    "#     yo k kati rakhne? experiment garne??\n",
    "    \n",
    "    \n",
    "# wait label smoothing kina garne ra????\n",
    "# hamle +1 gareko theyo ni inf huna nadina, yesma jati thulo + garyo teti label smoothing hunxa. \n",
    "\n",
    "# Jati thulo + garyo is simlar to increasing k.   (wow just wow)\n",
    "\n",
    "\n",
    "\n",
    "# Ok why do we did it?\n",
    "# ->hamro data perfectly accurate ta hunna ni ta, aba mathi data ma j paxi q aaune zero theyo, tara real life ma ta testo hudaina\n",
    "#   kunai na kunai ta hola j paxi q vako. so we add some number to consider those cases.\n",
    "    \n",
    "# Ok i found it just like bias.\n",
    "# Is label smoothing somewhat similar to adding a bias??\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ok learned new things \n",
    "\n",
    "# we got \n",
    "# loss = -probs[torch.arrange(num), ys].log().mean() + 0.01*(W**2).mean() + (i can add term here too, to experiment and make my model better)\n",
    "#                                                                           (if kei vaye loss decrease garna ni payo, or increase garna ni payo)\n",
    "# nice."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
